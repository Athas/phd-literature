\documentclass[a4paper, oneside, final]{memoir}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{mathtools}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\pdfminorversion=4
% bedre orddeling Gør at der som minimum skal blive to tegn på linien ved
% orddeling og minimum flyttes to tegn ned på næste linie. Desværre er værdien
% anvendt af babel »12«, hvilket kan give orddelingen »h-vor«.
\renewcommand{\britishhyphenmins}{22} 

% Fix of fancyref to work with memoir. Makes references look
% nice. Redefines memoir \fref and \Fref to \refer and \Refer.
% \usepackage{refer}             %
% As we dont really have any use for \fref and \Fref we just undefine what
% memoir defined them as, so fancyref can define what it wants.
\let\fref\undefined
\let\Fref\undefined
\usepackage{fancyref} % Better reference. 

\usepackage{pdflscape} % Gør landscape-environmentet tilgængeligt
\usepackage{fixme}     % Indsæt "fixme" noter i drafts.
\usepackage{hyperref}  % Indsæter links (interne og eksterne) i PDF

\usepackage[format=hang]{caption,subfig}
\usepackage{graphicx}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{ulem} % \sout - strike-through
\usepackage{tikz}

\renewcommand{\ttdefault}{txtt} % Bedre typewriter font
%\usepackage[sc]{mathpazo}     % Palatino font
\renewcommand{\rmdefault}{ugm} % Garamond
%\usepackage[garamond]{mathdesign}

%\overfullrule=5pt
%\setsecnumdepth{part}
\setcounter{secnumdepth}{1} % Sæt overskriftsnummereringsdybde. Disable = -1.
\chapterstyle{hangnum} % changes style of chapters, to look nice.

\makeatletter
\newenvironment{nonfloatingfigure}{
  \vskip\intextsep
  \def\@captype{figure}
  }{
  \vskip\intextsep
}

\newenvironment{nonfloatingtable}{
  \vskip\intextsep
  \def\@captype{table}
  }{
  \vskip\intextsep
}
\makeatother

\usepackage[
hyperref=auto,
backend=biber,
style=numeric,
defernumbers=true
]{biblatex}

\bibliography{bibliography}

\title{Literature Review for my PhD}

\author{Troels Henriksen (athas@sigkill.dk)}

\date{\today}
\pagestyle{plain}

\begin{document}

\frontmatter

\maketitle
\thispagestyle{empty}

Conferences to check out:

\begin{itemize}
\item PLDI
\item PACD
\item ICS
\item PPOPP
\item CGO
\item Compiler Construction
\item FHPC
\end{itemize}

People:

\begin{description}
\item[Polyhedral model]: Saday Sadayappan, Albert Cohen, Cédric
  Bastoul, F Irigoin Louis-/Noël/ Pouchet, Cosmin Oancea.
\end{description}

Stuff:

\begin{description}
\item[Rodinia]\hfill\\ A GPU-oriented benchmark suite.
\end{description}

\begin{quote}
  \fullcite{Bergstrom:2013:DFN:2517327.2442525}
\end{quote}

Multicore-focused, although it looks like it would work well on GPUs
as well.  Permits several representations of ``same'' data (mostly
regular and flattened) to coincide within program.  Not too dissimilar
to Futhark's idea of representing both optimised and non-optimised
cases.  Uses coercions, that are always reversible, to change
representations, and array operations that are overloaded with respect
to representation.  Not a bad idea.  I do not understand how they can
avoid having too many expensive representation changes.  In fact,
their own dense matrix multiplication example shows that they have a
problem.

\begin{quote}
  \fullcite{Sharma:2013:DEC:2544173.2509509}
\end{quote}

Comparing the equivalence of loops at the (x86) assembly level.  Can
be used to verify validity of optimisations, but you know.  Uses test
cases to guess relationships between loop fragments, which are then
attempted proven.

\begin{quote}
  \fullcite{Wienke:2012:OFE:2402420.2402522}
\end{quote}

Uses OpenACC and compares the resulting performance (both optimised
and non-optimised) to Portland Group/PGI and hand-optimised OpenCL.
All optimisation is here is manual, by modifying the OpenMP-like
annotations of ACC.  Performance is decent and the annotation style of
programming is apparently usable (if you like that sort of thing),
although on complex benchmarks, they come quite short of matching
OpenCL performance, which is ascribed to not using device-local
memory.  One nice property is that it's fairly easy to add naive
annotations, which can then be gradually improved and refined as
necessary, given knowledge of the target hardware.

We need to get our hands on their benchmark programs.

Low level approach.

\begin{quote}
  \fullcite{Reyes:2012:AOI:2402420.2402523}
\end{quote}

First implementation of OpenCL support for OpenACC, the OpenMP-style
standard for using accelerators.  Presents an OpenACC runtime
(``Frangollo'') that could be used by other compilers - maybe
non-OpenACC approaches as well?  The runtime supports both OpenCL and
CUDA, and for OpenCL, can make use of heteroneous devices - even just
multire CPUs - and respond intelligently.  Cannot do any real
automatic optimisation, it seems.  Benchmarks include molecular
dynamics.  Speedup seems good, although it's really hard to understand
their figures.

\begin{quote}
  \fullcite{Stock:2014:FED:2594291.2594342}
\end{quote}

\begin{quote}
  \fullcite{baghdadi2012pencil}
\end{quote}

PENCIL is an intermediate language designed similarly to OpenMP and
family, with pragmas put on top of C.  However, PENCIL severely
restricts the C dialect it supports in order to permit easier analysis
by an auto-parallelising compiler.  The semantics are still basically
sequential, but permitting annotations that can be used to indicate
independence.

Most interestingly, PENCIL supports ``access functions'', by which the
access patterns of a function are given by defining another function.
This other function uses some opaque macros that somehow indicate
which writes and reads are performed.  It's unclear how effective or
necessary this is in practice, but it's definitely a cool way to
indicate memory access patterns.

Unfortunately, this paper is pretty skimpy with the details.  It looks
a bit like a preprint... I should find some more information.

I found a presentation from HIPEAC'14 that suggests they get good
OpenCL speedups on a (very) simple kernel.

\begin{quote}
  \fullcite{Campanoni:2012:HAP:2259016.2259028}
\end{quote}

Presents HELIX, a technique for automatic loop parallelization that
assigns successive iterations of a loop to separate CPU threads.
Parallelises ordinary sequential programs (SPEC CPU2000 used as
benchmark).

Makes intelligent use of SMT (Simultanous Multi-Threading AKA
Hyper-Threading) by assigning each computation thread a ``helper
thread'' that does latency reduction by taking responsibility for
sending and receiving signals.

Each iteration of a parallel loop executed on a separate core, with
round-robin, but a trick is that the next iteration can be started
before the previous iteration is completely done, as long as data
dependencies have been satisfied.  In fact, as soon as the loop
prologue for one iteration has completed, the prologue for the next
iteration is begun.  For every data dependency $d$, a core will issue
a $wait(d)$ before it uses it, and a $signal(d)$ once it is done with
it.  This corresponds to sending signals between adjacent
threads/cores, and means that different iterations can interleave in
interesting ways.  It does mean that data dependencies between
non-adjacent threads have to be communicated through the intervening
threads.

Only one parallel loop can run at a time, so an important question is
which to parallelise.  They have an interesting algorithm based on
creating a loop hierarchy graph.  Not sure we can use it in Futhark,
but it's worth taking a closer look once we get closer.

This paper is quite good, and the technique is actually wonderfully
simple.

\begin{quote}
  \fullcite{6008968}
\end{quote}

This paper presents compilation schemes (although called
``transformations'') from high-level languages to GPU code (CUDA or
OpenCL).  The languages being inspected are ArrayOL, an image/signal
processing DSL, and Single Assignment C.

The paper describes how SaC compiles to CUDA: outermost
\texttt{with}-loops (essentially maps) become kernels, with all free
variables in the loop body transferred to the GPU before launch.  Very
straightforward approach, but seemingly with no effort made to take
advantage of GPU hardware details.  At least none that are mentioned.

Performance is good for the problem described in the paper (video
scaling).

Cite this when writing about kernel extraction.

\begin{quote}
  \fullcite{stavaaker2010compilation}
\end{quote}

Compiles Modelica to SaC, and SaC to CUDA.  Same approach as above.
Mentions that SaC cannot handle data-sensitive allocations in kernels
- unclear whether the allocation size is actually loop-variant, or
maybe just too big.  Demonstrates that SaC does not copy back and
forth between every kernel invocation, if the output of a kernel is
immediately (and solely) used as the input to another kernel.

Has a nice diagram showing how SaC \texttt{with}-loops are turned into
kernels and kernel invocations.

\begin{quote}
  \fullcite{jay1996shape}
\end{quote}

Presents a totally static approach to shape analysis, somewhat similar
to FISH (or however it is capitalised).  Defines a \textit{shapely
  program} as one whose result shape depends solely on the input
shapes - not their values.  Does not proceed (solely) by slicing, but
also by language features.  A type Sz for vector sizes is introduced,
and a corresponding shape-conditional branch construct.  For each term
$t : T$, the paper associates a shape term $\#t : \#T$.  Evaluation of
$\#t$ is simpler than $\#t$, but the point is that if $t$ has a shape
error, then so does $\#t$, and $\#t$ can be fully evaluated as soon as
the concrete shapes of inputs to $t$ are known.

This approach is not as strong as dependently typed systems, but it is
lightweight and easy to understand and implement.  Perhaps it could be
paired with constraint solving for detecting errors statically.

They mention that shape analysis of $fact 0$ will never terminate,
since it contains a data conditional.  I also do not see how something
like $filter$ could ever be handled this way, since it is technically
data-sensitive, but still with useful known invariants.

\section{HIPERFIT reading group papers}

\begin{quote}
  \fullcite{Trojahner2009643}
\end{quote}

A distilled presentation of Qube, which is a language/calculus that
uses a restricted (solvable) variant of dependent types to encode
shape/index invariants in the type system, and thusly provide runtime
guarantees.  Done by SaC people.  They support rank polymorphism in a
cool way, and value polymorphism seems like an easy extension.

Does not mention non-full indexing?  Same approach should work.

Permits irregular arrays through a really nifty \textit{dependent
  tuple} mechanism, where the type of the second element may depend on
the first element.  This is a necessary encoding since their type
system embeds shape information in arrays, and an irregular array
would otherwise be ill-typed.

Their branching construct imparts information to the type checker.
All branching is checking whether a value is in some range.

They call non-arrays (basic types, e.g. integers) \textit{quarks}
because they are ``subatomic'' (with arrays being atomic).  Note that
a tuple (possibly of arrays) is also a quark.

Instead of SOACs, they have just \textit{with}-loops (as in SaC),
which are general enough to represent \texttt{map}.  It is unclear how
this impacts fusion, and whether it is powerful enough to represent
other parallel primitives.  Of course, this is not the point of the
paper, and I am quite certain that the type system can support the
usual repertoire of SOACs.

Type checking uses constraint solvers to solve subset inclusion.

A PhD thesis exists that has newer information.

They do not permit multiplication in the index language, which is used
to represent shapes statically.  This means that e.g. a flatten
operation is not elegant.

\begin{quote}
  \fullcite{FromContractsTowardsDependentTypes}
\end{quote}

Describes SaC's approach to statically optimising runtime bounds
checks (and related) at compile-time, using partial evaluation.  Very
similar to Futhark's approach, although older, and more general, as
they also consider arrays whose rank is statically unknown (including
whether it is actually a scalar).

However, where Futhark uses the general language to compute
assertions, SaC uses a special fine-grained tailor-made functions to
compile what they call \textit{contracts}.  This is partially due to
efficiency concerns (as they possibly have far more checks than we
do), and partially due to some strange comment about the validation
code also needing validation, which may cause circularity.  I do not
buy this latter argument, or maybe I just do not understand it.

They are able to derive knowledge from evaluating their contracts,
which is nice.

The paper goes through various designs of contracts, and end up with
something that is very much like the one in Futhark, with explicit
evidence (what is in Futhark called ``certificates'').  And this was
written five years before I came up with Futhark's design!  Convergent
evolution tends to suggest that something is at least a local maximum.

The approach in the paper is more fleshed out, however, and applies to
every construct in the language (which is not the case in Futhark as
of this writing).

The use of \texttt{after\_guard} seems a little clumsy and fragile,
and in fact limits an optimisation on page 14, where, conceptually,
\texttt{let a[i] = v in a[i]} should simplify to \texttt{v}
\textit{but keep the bounds checks as a pre/post-condition for
  validity}.  This is something I will do better once I flesh out the
system in Futhark.

There is a (sketch) proof of validity, but honestly, it is quite
obvious to me that their technique is sound.

I did not know that such terms as \textsc{common subexpression
  elimination} and \textsc{function inlining} deserved smallcaps.

When the paper mentions they use ``partial evaluation'' to discharge
the assertions statically, this really means normal small-scale
optimisations, like constant folding and shape inference.  This is
fine, and the same that Futhark does, but I had hoped for something
more sophisticated.

We must cite this paper soon.  Even if I had no about it at the time I
developed the Futhark assertion system, the similarities are striking.

The main difference is that in Futhark, we also have a strong type
system component (for array sizes), with certificates/contracts used
to ensure that \texttt{reshape} operations are valid.

\begin{quote}
  \fullcite{AnArrayOrientedLanguageWithStaticRankPolymorphism}
\end{quote}

The authors present a language, Remora, that is essentially typed APL
or J.  Only a core language; not particularly pleasant to write in.
Has only regular arrays, which they claim is ``like APL''.  Must be a
matter of perspective of the frame/cell definition.  Quite nice
explanation of the frame/cell composition and automatic
replication. Does the term ``unspecified rank'' have a specific
meaning?  I think it just means ``arbitrary''.

Conclusion: fairly standard approach with a restricted language for
dependent types, but it is interesting that it codifies the
automatic/implicit array expansion found in J.

\begin{quote}
  \fullcite{Xi:1999:DTP:292540.292560}
\end{quote}

A condensed (extended abstract) presentation of Dependent ML.  This is
basically the OG of practically useful restricted dependent types, and
should be the primary inspiration if we decide to go in this
direction.  Essentially all of ML is supported, which is more than
what I need.  Superficially, the approach seems very similar to
Haskell GADTs, but with the type-checker aware of the constraint
domain, where it presumably does something intelligent via constraint
solving.  The types of the constraint language are called
\textit{index sorts} - this has confused me in previous papers, but it
must have been an established term at least since this paper, and we
should do the same in Futhark.

What does ``conservativity over ML'' mean?  That it does not affect
operational semantics?  That it has erasure?

Important point. DML does not make more programs typeable than ML, but
tries to assign more accurate types to existing programs (and maybe
rejecting some of them).

The approach is that of an explicitly typed internal language
$ML_{0}^{\Pi}(C)$, and an implicitly typed external language, with the
latter being transformed through elaboration into the former.  This is
also what we do in Futhark, except that our internal language is much
less principled.

(What is the constraint language $C$?)

Adds the notion of existential dependent types (also called weak
dependent sums) to express non-strict invariants, such as the size of
the return of \texttt{filter}.

Encodes arbitrary propositions $P$ as singletons of sort $\{\ ()\ :\
unit\ |\ P\ \}$ - if an index variable of this type is in scope, then
$P$ must be true!  Nice and elegant.

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
