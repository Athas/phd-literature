\documentclass[a4paper, oneside, final]{memoir}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{mathtools}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\pdfminorversion=4
% bedre orddeling Gør at der som minimum skal blive to tegn på linien ved
% orddeling og minimum flyttes to tegn ned på næste linie. Desværre er værdien
% anvendt af babel »12«, hvilket kan give orddelingen »h-vor«.
\renewcommand{\britishhyphenmins}{22} 

% Fix of fancyref to work with memoir. Makes references look
% nice. Redefines memoir \fref and \Fref to \refer and \Refer.
% \usepackage{refer}             %
% As we dont really have any use for \fref and \Fref we just undefine what
% memoir defined them as, so fancyref can define what it wants.
\let\fref\undefined
\let\Fref\undefined
\usepackage{fancyref} % Better reference. 

\usepackage{pdflscape} % Gør landscape-environmentet tilgængeligt
\usepackage{fixme}     % Indsæt "fixme" noter i drafts.
\usepackage{hyperref}  % Indsæter links (interne og eksterne) i PDF

\usepackage[format=hang]{caption,subfig}
\usepackage{graphicx}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{listings}
\usepackage[normalem]{ulem} % \sout - strike-through
\usepackage{tikz}

\renewcommand{\ttdefault}{txtt} % Bedre typewriter font
%\usepackage[sc]{mathpazo}     % Palatino font
\renewcommand{\rmdefault}{ugm} % Garamond
%\usepackage[garamond]{mathdesign}

%\overfullrule=5pt
%\setsecnumdepth{part}
\setcounter{secnumdepth}{1} % Sæt overskriftsnummereringsdybde. Disable = -1.
\chapterstyle{hangnum} % changes style of chapters, to look nice.

\makeatletter
\newenvironment{nonfloatingfigure}{
  \vskip\intextsep
  \def\@captype{figure}
  }{
  \vskip\intextsep
}

\newenvironment{nonfloatingtable}{
  \vskip\intextsep
  \def\@captype{table}
  }{
  \vskip\intextsep
}
\makeatother


\usepackage[
hyperref=auto,
backend=biber,
style=numeric-comp,
defernumbers=true
]{biblatex}

% The following two declarations gives me underlined and properly
% wrapped titles in the \fullcites.
\DeclareFieldFormat*{title}{#1}
\DeclareFieldFormat*{titlecase}{
    \ifdef{\currentfield}
      {\ifcurrentfield{title}
         {\usefield{\uline}{\currentfield}}%
         {#1}}
      {#1}}

\addbibresource{bibliography.bib}

\title{Literature Review for my PhD}

\author{Troels Henriksen (athas@sigkill.dk)}

\date{\today}
\pagestyle{plain}

\begin{document}

\frontmatter

\maketitle

Conferences to check out:

\begin{itemize}
\item PLDI
\item PACD
\item ICS
\item PPOPP
\item CGO
\item Compiler Construction
\item FHPC
\end{itemize}

People:

\begin{description}
\item[Polyhedral model]: Saday Sadayappan, Albert Cohen, Cédric
  Bastoul, F Irigoin Louis-/Noël/ Pouchet, Cosmin Oancea.
\end{description}

Stuff:

\begin{description}
\item[Rodinia]\hfill\\ A GPU-oriented benchmark suite.
\end{description}

\section*{Assorted paper notes}

\begin{quote}
  \fullcite{Bergstrom:2013:DFN:2517327.2442525}
\end{quote}

Multicore-focused, although it looks like it would work well on GPUs
as well.  Permits several representations of ``same'' data (mostly
regular and flattened) to coincide within program.  Not too dissimilar
to Futhark's idea of representing both optimised and non-optimised
cases.  Uses coercions, that are always reversible, to change
representations, and array operations that are overloaded with respect
to representation.  Not a bad idea.  I do not understand how they can
avoid having too many expensive representation changes.  In fact,
their own dense matrix multiplication example shows that they have a
problem.

\begin{quote}
  \fullcite{Sharma:2013:DEC:2544173.2509509}
\end{quote}

Comparing the equivalence of loops at the (x86) assembly level.  Can
be used to verify validity of optimisations, but you know.  Uses test
cases to guess relationships between loop fragments, which are then
attempted proven.

\begin{quote}
  \fullcite{Wienke:2012:OFE:2402420.2402522}
\end{quote}

Uses OpenACC and compares the resulting performance (both optimised
and non-optimised) to Portland Group/PGI and hand-optimised OpenCL.
All optimisation is here is manual, by modifying the OpenMP-like
annotations of ACC.  Performance is decent and the annotation style of
programming is apparently usable (if you like that sort of thing),
although on complex benchmarks, they come quite short of matching
OpenCL performance, which is ascribed to not using device-local
memory.  One nice property is that it's fairly easy to add naive
annotations, which can then be gradually improved and refined as
necessary, given knowledge of the target hardware.

We need to get our hands on their benchmark programs.

Low level approach.

\begin{quote}
  \fullcite{Reyes:2012:AOI:2402420.2402523}
\end{quote}

First implementation of OpenCL support for OpenACC, the OpenMP-style
standard for using accelerators.  Presents an OpenACC runtime
(``Frangollo'') that could be used by other compilers - maybe
non-OpenACC approaches as well?  The runtime supports both OpenCL and
CUDA, and for OpenCL, can make use of heteroneous devices - even just
multire CPUs - and respond intelligently.  Cannot do any real
automatic optimisation, it seems.  Benchmarks include molecular
dynamics.  Speedup seems good, although it's really hard to understand
their figures.

\begin{quote}
  \fullcite{baghdadi2012pencil}
\end{quote}

PENCIL is an intermediate language designed similarly to OpenMP and
family, with pragmas put on top of C.  However, PENCIL severely
restricts the C dialect it supports in order to permit easier analysis
by an auto-parallelising compiler.  The semantics are still basically
sequential, but permitting annotations that can be used to indicate
independence.

Most interestingly, PENCIL supports ``access functions'', by which the
access patterns of a function are given by defining another function.
This other function uses some opaque macros that somehow indicate
which writes and reads are performed.  It's unclear how effective or
necessary this is in practice, but it's definitely a cool way to
indicate memory access patterns.

Unfortunately, this paper is pretty skimpy with the details.  It looks
a bit like a preprint... I should find some more information.

I found a presentation from HIPEAC'14 that suggests they get good
OpenCL speedups on a (very) simple kernel.

\begin{quote}
  \fullcite{Campanoni:2012:HAP:2259016.2259028}
\end{quote}

Presents HELIX, a technique for automatic loop parallelization that
assigns successive iterations of a loop to separate CPU threads.
Parallelises ordinary sequential programs (SPEC CPU2000 used as
benchmark).

Makes intelligent use of SMT (Simultanous Multi-Threading AKA
Hyper-Threading) by assigning each computation thread a ``helper
thread'' that does latency reduction by taking responsibility for
sending and receiving signals.

Each iteration of a parallel loop executed on a separate core, with
round-robin, but a trick is that the next iteration can be started
before the previous iteration is completely done, as long as data
dependencies have been satisfied.  In fact, as soon as the loop
prologue for one iteration has completed, the prologue for the next
iteration is begun.  For every data dependency $d$, a core will issue
a $wait(d)$ before it uses it, and a $signal(d)$ once it is done with
it.  This corresponds to sending signals between adjacent
threads/cores, and means that different iterations can interleave in
interesting ways.  It does mean that data dependencies between
non-adjacent threads have to be communicated through the intervening
threads.

Only one parallel loop can run at a time, so an important question is
which to parallelise.  They have an interesting algorithm based on
creating a loop hierarchy graph.  Not sure we can use it in Futhark,
but it's worth taking a closer look once we get closer.

This paper is quite good, and the technique is actually wonderfully
simple.

\begin{quote}
  \fullcite{6008968}
\end{quote}

This paper presents compilation schemes (although called
``transformations'') from high-level languages to GPU code (CUDA or
OpenCL).  The languages being inspected are ArrayOL, an image/signal
processing DSL, and Single Assignment C.

The paper describes how SaC compiles to CUDA: outermost
\texttt{with}-loops (essentially maps) become kernels, with all free
variables in the loop body transferred to the GPU before launch.  Very
straightforward approach, but seemingly with no effort made to take
advantage of GPU hardware details.  At least none that are mentioned.

Performance is good for the problem described in the paper (video
scaling).

Cite this when writing about kernel extraction.

\begin{quote}
  \fullcite{stavaaker2010compilation}
\end{quote}

Compiles Modelica to SaC, and SaC to CUDA.  Same approach as above.
Mentions that SaC cannot handle data-sensitive allocations in kernels
- unclear whether the allocation size is actually loop-variant, or
maybe just too big.  Demonstrates that SaC does not copy back and
forth between every kernel invocation, if the output of a kernel is
immediately (and solely) used as the input to another kernel.

Has a nice diagram showing how SaC \texttt{with}-loops are turned into
kernels and kernel invocations.

\begin{quote}
  \fullcite{jay1996shape}
\end{quote}

Presents a totally static approach to shape analysis, somewhat similar
to FISH (or however it is capitalised).  Defines a \textit{shapely
  program} as one whose result shape depends solely on the input
shapes - not their values.  Does not proceed (solely) by slicing, but
also by language features.  A type Sz for vector sizes is introduced,
and a corresponding shape-conditional branch construct.  For each term
$t : T$, the paper associates a shape term $\#t : \#T$.  Evaluation of
$\#t$ is simpler than $\#t$, but the point is that if $t$ has a shape
error, then so does $\#t$, and $\#t$ can be fully evaluated as soon as
the concrete shapes of inputs to $t$ are known.

This approach is not as strong as dependently typed systems, but it is
lightweight and easy to understand and implement.  Perhaps it could be
paired with constraint solving for detecting errors statically.

They mention that shape analysis of $fact 0$ will never terminate,
since it contains a data conditional.  I also do not see how something
like $filter$ could ever be handled this way, since it is technically
data-sensitive, but still with useful known invariants.

\begin{quote}
  \fullcite{Brodman:2009:NAD:1855591.1855607}
\end{quote}

The authors present suggestions on data-parallel constructs, and the
design of data-parallel programming languages.  One significant point
is to describe a program as a sequence of parallel operations, such
that the meaning of the program can be determined by ignoring the
parallelism.  Parallelism thus becomes an operational issue, not a
semantic one, as is proper.  The authors do not concern themselves
overmuch with compiler optimisation, and rather focus on the selection
of portable parallel ``operators'' that can be efficiently implemented
on variety of platforms.

The authors find inspiration on the selection of operators in three
sources:

\begin{itemize}
\item Old functional language constructs (\texttt{map},
  \texttt{reduce}) that are ``incidentally'' parallel (sometimes).
\item Vector hardware and instructions.
\item The data-parallel operators of existing high-level languages.
\end{itemize}

Most of these operate on flat data structures, with NESL as the
notable exception.  The authors argue that this is insufficient, and
present a concept of tiled parallel arrays as a programming model
(they also mention the need sets, but they do not discuss how these
could be decomposed).  The nested structure is mostly presented as a
way to perform dynamic scheduling and load balancing, and less as a
means of expressivity, but maybe that is implied.

\section*{HIPERFIT reading group papers}

\begin{quote}
  \fullcite{Trojahner2009643}
\end{quote}

A distilled presentation of Qube, which is a language/calculus that
uses a restricted (solvable) variant of dependent types to encode
shape/index invariants in the type system, and thusly provide runtime
guarantees.  Done by SaC people.  They support rank polymorphism in a
cool way, and value polymorphism seems like an easy extension.

Does not mention non-full indexing?  Same approach should work.

Permits irregular arrays through a really nifty \textit{dependent
  tuple} mechanism, where the type of the second element may depend on
the first element.  This is a necessary encoding since their type
system embeds shape information in arrays, and an irregular array
would otherwise be ill-typed.

Their branching construct imparts information to the type checker.
All branching is checking whether a value is in some range.

They call non-arrays (basic types, e.g. integers) \textit{quarks}
because they are ``subatomic'' (with arrays being atomic).  Note that
a tuple (possibly of arrays) is also a quark.

Instead of SOACs, they have just \textit{with}-loops (as in SaC),
which are general enough to represent \texttt{map}.  It is unclear how
this impacts fusion, and whether it is powerful enough to represent
other parallel primitives.  Of course, this is not the point of the
paper, and I am quite certain that the type system can support the
usual repertoire of SOACs.

Type checking uses constraint solvers to solve subset inclusion.

A PhD thesis exists that has newer information.

They do not permit multiplication in the index language, which is used
to represent shapes statically.  This means that e.g. a flatten
operation is not elegant.

\begin{quote}
  \fullcite{FromContractsTowardsDependentTypes}
\end{quote}

Describes SaC's approach to statically optimising runtime bounds
checks (and related) at compile-time, using partial evaluation.  Very
similar to Futhark's approach, although older, and more general, as
they also consider arrays whose rank is statically unknown (including
whether it is actually a scalar).

However, where Futhark uses the general language to compute
assertions, SaC uses a special fine-grained tailor-made functions to
compile what they call \textit{contracts}.  This is partially due to
efficiency concerns (as they possibly have far more checks than we
do), and partially due to some strange comment about the validation
code also needing validation, which may cause circularity.  I do not
buy this latter argument, or maybe I just do not understand it.

They are able to derive knowledge from evaluating their contracts,
which is nice.

The paper goes through various designs of contracts, and end up with
something that is very much like the one in Futhark, with explicit
evidence (what is in Futhark called ``certificates'').  And this was
written five years before I came up with Futhark's design!  Convergent
evolution tends to suggest that something is at least a local maximum.

The approach in the paper is more fleshed out, however, and applies to
every construct in the language (which is not the case in Futhark as
of this writing).

The use of \texttt{after\_guard} seems a little clumsy and fragile,
and in fact limits an optimisation on page 14, where, conceptually,
\texttt{let a[i] = v in a[i]} should simplify to \texttt{v}
\textit{but keep the bounds checks as a pre/post-condition for
  validity}.  This is something I will do better once I flesh out the
system in Futhark.

There is a (sketch) proof of validity, but honestly, it is quite
obvious to me that their technique is sound.

I did not know that such terms as \textsc{common subexpression
  elimination} and \textsc{function inlining} deserved smallcaps.

When the paper mentions they use ``partial evaluation'' to discharge
the assertions statically, this really means normal small-scale
optimisations, like constant folding and shape inference.  This is
fine, and the same that Futhark does, but I had hoped for something
more sophisticated.

We must cite this paper soon.  Even if I had no about it at the time I
developed the Futhark assertion system, the similarities are striking.

The main difference is that in Futhark, we also have a strong type
system component (for array sizes), with certificates/contracts used
to ensure that \texttt{reshape} operations are valid.

\begin{quote}
  \fullcite{AnArrayOrientedLanguageWithStaticRankPolymorphism}
\end{quote}

The authors present a language, Remora, that is essentially typed APL
or J.  Only a core language; not particularly pleasant to write in.
Has only regular arrays, which they claim is ``like APL''.  Must be a
matter of perspective of the frame/cell definition.  Quite nice
explanation of the frame/cell composition and automatic
replication. Does the term ``unspecified rank'' have a specific
meaning?  I think it just means ``arbitrary''.

Conclusion: fairly standard approach with a restricted language for
dependent types, but it is interesting that it codifies the
automatic/implicit array expansion found in J.

\begin{quote}
  \fullcite{Xi:1999:DTP:292540.292560}
\end{quote}

A condensed (extended abstract) presentation of Dependent ML.  This is
basically the OG of practically useful restricted dependent types, and
should be the primary inspiration if we decide to go in this
direction.  Essentially all of ML is supported, which is more than
what I need.  Superficially, the approach seems very similar to
Haskell GADTs, but with the type-checker aware of the constraint
domain, where it presumably does something intelligent via constraint
solving.  The types of the constraint language are called
\textit{index sorts} - this has confused me in previous papers, but it
must have been an established term at least since this paper, and we
should do the same in Futhark.

What does ``conservativity over ML'' mean?  That it does not affect
operational semantics?  That it has erasure?

Important point. DML does not make more programs typeable than ML, but
tries to assign more accurate types to existing programs (and maybe
rejecting some of them).

The approach is that of an explicitly typed internal language
$ML_{0}^{\Pi}(C)$, and an implicitly typed external language, with the
latter being transformed through elaboration into the former.  This is
also what we do in Futhark, except that our internal language is much
less principled.

(What is the constraint language $C$?)

Adds the notion of existential dependent types (also called weak
dependent sums) to express non-strict invariants, such as the size of
the return of \texttt{filter}.

Encodes arbitrary propositions $P$ as singletons of sort $\{\ ()\ :\
unit\ |\ P\ \}$ - if an index variable of this type is in scope, then
$P$ must be true!  Nice and elegant.

\begin{quote}
  \fullcite{Stock:2014:FED:2594291.2594342}
\end{quote}

\textit{This paper has a really nice style: they present the runtime
  for the unoptimised program, then the optimised program, already on
  page 2 before they go into any details.  It is very clear what they
  are trying to solve, and what results they get.}

Exploiting the associativity of operations\footnote{...and assuming
  that floating point operations are associative.} involved in stencil
computations, the authors reorder the operations to enhance register
usage and lower memory pressure.  This is particularly useful for
higher-order stencils (``weighted averages over multiple neighboring
points along each dimension''), as these have high arithmetic
intensity.  These ought to be more efficient as computation dominates
memory access, but due to register pressure, an excessive amount of
spilling happens.

This could also be useful for GPGPU: whilst there is no register
spilling per se (everything is basically a L1 cache), minimising
register usage increases occupancy, and memory accesses are super
slow.

A modern multicore computer can provide 0.25 bytes/flop.  An example
$3 \times 3$ (order 1) stencil requires 16 bytes per 17 flops, and so
is bandwidth-bound.  A $5 \times 5$ (order 2) convolution has a ratio
of 32/100, and so is probably also bandwidth-bound.  But a $7 \times
7$ (order 3) stencil will be roughly half that, that is, 16/100, and
so should be CPU-bound.  It is not, because the register pressure
worsens, causing each value to be reloaded from memory many times.
This is due to inner (unrolled) loops where the compiler reuses
registers to avoid loads, but eventually runs out of registers (order
2 requires 20 and order 3 requires 42), leading to spills.

Why do they write ``a distinct C code''?  It should just be ``distinct
C code''.

It is not enough to simply reorder the operations accumulating in a
single output point, instead, we must interleave the accumulation of
different output points.  Reordering operations is the only change,
however.  In their example, the original code is an ``all-gather''
stencil, and the optimised version is ``scatter-gather''.  For an $n
\times n$-stencil, the transformed version involves a $n \times 1$
read-set updating a $1 \times n$ write-set in an all-to-all fashion.
Compared to the immediate version, this reduces register pressure from
$n^{2}$ to $n$ (approximately).  Specifically, the number of memory
stores is significantly increased, but the number of loads is
dramatically decreased.

The stencil representation is extracted using polyhedral concepts,
after which an analytical model is used to prune scatter/gather
variants, code generated, and finally machine-specific autotuning
performed.

``Multidimensional retiming is in essence a shift of an iteration set
by a constant quantity along one or more of its dimensions.''  The
idea seems to be to tag each statement with its \textit{schedule}, a
description of when it is executed within the iteration space, then to
apply a \textit{retiming vector} that linearly moves the statement.
If the entire program region to optimise fits the polyhedral model,
polyhedral dependence analysis can be used to determine whether a
specific retiming vector preserves the relative order of all dependent
statements.

A central idea seems to be that even if one statement depends on
another, if they have a certain structure (in-place update with
associative operator), their dependence commutes, and they can
therefore be reordered.  For convolution stencils, all dependencies
are commutative.

It is unclear whether this matters on the GPU.  The authors have
previous GPU experience, yet never talk about this in the paper.  The
setting is purely sequential and the optimisation directly assumes
sequential execution.

\begin{quote}
  \fullcite{McDonell:2013:OPF:2500365.2500595}
\end{quote}

Main/original Accelerate paper.

Lots on sharing, which is not relevant to non-embedded languages.  The
approach seems good if we ever need to do anything in that direction.
There is a lesson here: the paper contributes two very different
solutions - one for recovering sharing for embedded DSLs, and one for
optimising producer-consumer loops.  The intersection of audiences for
these two is very small.

Filter is decomposed into primitives (the usual map-scan-permute
song-and-dance), but these cannot be fused, as they are too primitive.

It is unusual to attribute the terms ``producer'' and ``consumer'' to
language constructs - these are usually applied to the operations
involved in a concrete fusion operation.  ``Generators'' and
``reducers'' may be better terms.

The important constructor in their representation of delayed arrays
(\texttt{DelayedArray}) is the constructor \texttt{Step}, whick takes
an underlying array, and performs an arbitrary index-space and
value-space transformation.  Cool and flexible.  They also have
\texttt{Yield}, which is a function from index to value, so less
information.

Accelerate provides two functions for mapping index to value - one
with flat indexes, and one with nested indexes.  This is presumably
useful for avoiding modulo and division operations.

\begin{quote}
  \fullcite{clifton2014embedding}
\end{quote}

Presents the skeleton instantiation/code generation scheme of
Accelerate.  Mostly in the style of ``we wrote this code, now we will
explain how it works''.  Quite nice, and more people should write
stuff like that, but it is pretty basic, with little new information.

The section on dead-code elimination is truly bizarre.  Why don't they
just do normal dead-code elimination?  Maybe due to the delayed or
skeleton approach?

\begin{quote}
  \fullcite{Feautrier96automaticparallelization}
\end{quote}

Presents parallelism optimisation with the polytope model.  Quite a
nice basic introduction.  Defines the idea of finding a partial order
$\prec_{//}$ for which any extension (turning it total by adding more
constraints, i.e, defining actual execution order) gives the same
result as the original program.  In a program-as-written we are given
a strict total order (the order that instructions are written).
Clearly not parallel (except if the language has some built-in
parallelism...).  But some statements are for-real dependent,
specified with the $\delta$ relation.  Thus, the coarsest valid
partial order is
\[
\prec_{//} = (\prec\ \cap\ \delta)^{+}
\]

Of course, $\delta$ is typically not feasible to compute.  Except in a
functional data-parallel program, where it is quite trivial.

The application in the paper is for \texttt{do}-loop nests.  The basic
idea is for each statement \texttt{S} to be associated with an
iteration vector \texttt{I} that contains the loop indices for
surrounding loops, yielding the tuple \texttt{$\langle$S, I$\rangle$}.
The vector is constrained by the loop bounds.  If these are affine
functions of the surrounding loop counters and structure parameters
(array sizes?), then the iteration domain of statement \texttt{S} is
an integer vector inside a Z-polytope.

\begin{quote}
  \fullcite{Holk:2014:RMM:2660193.2660244}
\end{quote}

This is a wonderful paper!  Good work, and a pleasure to read.  A
shame the empirical evaluation is terrible and the weaknesses of the
approach are not presented well.

Presents Harlan, a high-level language for GPGPU with Scheme syntax
that supports ADTs.  Harlan makes use of region-based memory
management to support transfor of data structures from CPU to GPU.  It
supports hygienic macros, which is probably the way to go for these
kinds of languages.

The work is motivated by the observation that GPUs and CPUs are
converging in architecture, and thus a programming high-level model
covering both should be devised.  The authors mention that because
Harlan compiles to OpenCL, it is trivial to switch between CPU and GPU
execution.  This is only the case if you are willing to accept sub-par
performance.

Uses the language construct \texttt{kernel} for a
\texttt{map}/texttt{zipWith}.  Seemingly no other parallel constructs.
Harlans permits apparently arbitrary IO, but compilation will fail if
these are put inside of a \texttt{kernel} function.

``the reduction operation is implemented as a macro in Harlands
runtime library'' - uh?  I wonder if they mean a language-level macro.

Irregular arrays are supported, as well as ADTs permitting DAGs and
trees.  Cycles are not permitted I guess?  Seemingly free-form
\texttt{lambda} forms are also permitted.  In fact, kernels can
produce closures which are then applied on the host!  You can also
pass e.g. vectors of functions into a kernel, which is fairly cool -
they term this ``multiple-program single-data''.  This is implemented
by defunctionalisation, where each \texttt{lambda} gives rise to one
constructor in an AST, with value parameters corresponding to the
captured variables.  A concrete closure is then just a value with this
constructor.  This means that application involves serious branching,
proportional with the number of different \texttt{lambda}s of that
type.  The branch divergence will be horrible.  The authors mention
that this can be limited with flow analysis, but that it has not been
a problem yet.  I think they are nowhere near peak GPU performance
with this design.  Perhaps this humble ambition is why they consider a
single programming model to be suitable for both GPU and CPU?

The memory region system is used to easily copy data structures from
CPU to GPU - since it is known exactly in which regions a structure is
stored, these can be copied immediately, instead of following
pointers.  The better the region inference, the more fine-grained this
copying can be, I think.  This region inference system is more
flexible than Futharks memory blocks, but it really solves a different
problem, I think.  And it may be more difficult to do allocation
analysis.  There are some strange properties in the region system, and
I wonder whether it is expressive enough, but I am not an expert in
the matter.  For example, in a closure, the entire environment must
fit in the same region.  Does this mean that we often end copying
between regions if we end up with contradictions?  Such a
contradiction can only occur if two values is constrained to two
different regions, but I don't think that will ever happen - in the
extreme end, everything can be put in the same region.  Of course,
this is bad, since the region is the unit of transfer between CPU and
GPU, and garbage collection is not done within regions (yet?).
Regions are resized on-demand, so there is a bit of indirection here -
a region is its size and a pointer to the block.  Since allocation is
not possible within a kernel, the program will fail at runtime if the
region does not have enough space available.  How do they handle race
conditions where multiple threads of the same kernel will try to
allocate from the same region?  And how do they handle tail-calls
within a region allocation?

The Harland compiler is implemented in Scheme in a nanopass style.
This is a wonderful style, but it does not seem feasible in a
statically typed compiler implementation language - at least not if
you want to change the annotations.  But maybe I can do something...

ADTs are compiled to C unions.  I am not sure how recursive ADTs are
handled, or whether they work well on the GPU.  \texttt{match} becomes
chains of \texttt{if} statements, which has horrible branch divergence
properties.

Nested kernels are sequentialised.  The outer kernel is always
preserved as parallel.  Apparently no distribution or flattening.
That is very bad.  In fact, it is the same approach as taken by the
Futhark prototype OpenCL backend - proof of concept, but nowhere near
production-ready.  Although they perform several other optimisations
(simple flattening and lazy copying from GPU to CPU) that greatly
improve the situation.

In OpenCL, recursive functions are translated to \texttt{goto} and
explicit stacks (``trampolining'', although they do not use this
term).  As far as I know, this is basically what you have to do.

Simple producer-consumer fusion between \texttt{kernel}s
(=\texttt{map}) is done, as well as simple
\texttt{kernel}-\texttt{reduce} fusion and flattening of
\texttt{kernel} nests.

The evaluation is weak.  Some trivial microbenchmarks (vector addition
and dot product) are compared with highly-optimised CUBLAS, where
Harlam does badly in the former and well in the latter.  For N-Body
simulation, the comparison is with Accelerate, not a hand-written
optimised CUDA version.  They do beat Accelerate.  The ray tracing
example is not compared with anything, but simply shows how execution
time scales with number of nodes.

Overall, it is admirable to construct a programming language that is
as flexible as traditional sequential languages, yet still permit
fairly transparent execution on GPU.  However, I do not believe that
their compilation strategy is able to take full advantage of the GPU
hardware.  Branch divergence is a serious cost, and efficient used of
shared memory and coalesced access is necessary to get good
performance from GPU.  While it is possible to get code to run at all
on the GPU, this does not mean that it will be efficient.  Thus, I do
not believe that Harland is a convincing argument that a unified
programming model is feasible - or at least, this model is not.

\begin{quote}
  \fullcite{rodinia}
\end{quote}

Presents Rodinia, a moderm parallel benchmark suite.  The suite
consists of GPU and CPU implementations of fairly simple programs.
The paper claims ``four applications and five kernels'' (unclear on
the specific difference), but the Rodinia 3.0 tarball I fetched has 18
benchmark directories, including OpenCL implementation, whilst the
paper only claims CUDA.

``Berkeley Dwarfs'' are used to guide the choice of benchmarks.  A
dwarf is defined as ``a dwarf is an algorithmic method that captures a
pattern of computation and communication''.  Rodinia does not (in this
paper) have a benchmark for every dwarf, but some dwarfs have multiple
benchmarks.

The benchmarks are very varied in their quality of implementation, but
I generally do not think they are written very well.  They can be hard
to understand due to including (far) too much irrelevant code, and
sometimes due to manual constant-folding.  For some reason, the paper
does not do a good job of describing what the benchmarks \textit{do},
so here are my own notes:

\begin{description}
\item[HotSpot] Fairly trivial rank-1 2d stencil, although with weird
  edge conditions.
\item[Kmeans] Convergence loop, body is a histogram, with reduction of
  the points falling in each of the buckets (but we must also know the
  size of each bucket).
\item
\end{description}

\begin{quote}
  \fullcite{Scholz:2003:SAC:967499.967501}
\end{quote}

A fairly big SaC paper from 2003.  Presents some of the problems
involved with doing high-performance computing in functional
languages, notably the problem with random-access arrays.  The paper
does not seem particularly focused on parallelism.  The focus on
efficient compilation is very admirable.  The authors argue for the
value of maintaining a syntax similar to C:

\begin{quote}
  The choice of a C-like syntax is motivated by two
  considerations. First, it allows programmers with an imperative
  background to have a smooth transition into the functional
  paradigm. Second, it facilitates the compilation process, since it
  allows some program parts to be mapped directly to C.
\end{quote}

I find these to considerations very dubious.  First, one cannot
pretend that a language is imperative merely because it has curly
braces.  Second, no non-trivial compilation can simply map to another
target language, and given how simple C is, those features could be
simply mapped to any other low-level target language.  Of course,
there is no doubt whatsoever that it is a massive advantage for a
language to have curly braces.

SaC also possesses an interesting type system supporting array rank
polymorphism, as well as a module system and FFI.  One of the most
impressive aspects of SaC is indeed that it feels like a complete
language.

Two interesting quotes:

\begin{quote}
  The most challenging design problem in this context is to let shape
  inference accept all shape correct programs without making the type
  system undecidable.
\end{quote}

and

\begin{quote}
  As a consequence, the type system turns out to be ambiguous wrt. the
  shape information, i.e., the extent to which shapes are statically
  inferred does not depend on the type system but on the
  implementation of the type inference algorithm actually applied.
\end{quote}

This is very similar to the Futhark approach!  Except, of course, that
Futhark does not support rank polymorphism, so our problem is simpler.
Then again, Futhark is a core language.

The SaC \texttt{cat} (concatenation) construct seems pretty powerful.

The most general array building block of SaC is the
\texttt{with}-loop, which seems inspired by the loop construct of
SISAL.  Three forms of \texttt{with}-loops are defined:

\begin{description}
\item[genarray] which creates arrays from an index space (so something
  like \texttt{map} over \texttt{iota}).
\item[modarray] which modifies existing arrays (general \texttt{map}?).
\item[foldarray] which performs reductions/foldings.
\end{description}

\texttt{scan} appears missing, and I am skeptical that
\texttt{with}-loop-folding (fusion/deforestation -- actually
\texttt{map}-\texttt{map} fusion) is as powerful as the fusion algebra
possible with more high-level SOACs.  However, \texttt{with}-loops
interact with the array rank polymorphism in a supremely nice way that
SOACs do not.  Like Futhark, SaC also makes it the programmers
responsibility to specify an associative and commutative function for
reduction.  \texttt{modarray} can express some things that are
inconvenient in Futhark, e.g:

\begin{verbatim}
double[*] EmbeddedAdd( double[*] small, int[.] offset, double[*] large)
{
  res = with( offset <= iv < offset + shape(small))
  modarray( large, iv, large[iv] + small[iv-offset]);
  return( res);
}
\end{verbatim}

This modifies the elements of \texttt{large} in a specified interval.

As an aside, the similarity of SaC to C breaks down rapidly from here
on.  I think the choice of name is poor.

I did not study the type system that closely, but it seems decent.

Apparently, the SaC compiler is able to reorder the iteration space of
\texttt{with}-loops to optimise cache locality, at least when the loop
nesting depth can be statically determined.  Reference counting is
used to execute \texttt{modarray}s in-place when possible.

This is again odd:

\begin{quote}
  Last not least, the strong syntactical similarity between the two
  languages allows the compilation efforts to be concentrated on
  adequate array representations and on optimizing the code for array
  operations.
\end{quote}

I do wonder how many of the oddities of C have been adopted.  The
scalar language itself is trivial, so I have a hard time believing it
is much of a win.

Can \texttt{with}-loops be nested?  There is no way to write it
explicitly, but fusion or program composition may cause them to arise,
in which case distribution or flattening could be useful.

\texttt{With}-loop-folding has some interesting quirks compared to
normal \texttt{map}-\texttt{map} fusion, due to the fact that
\texttt{with}-loops can be over just a slice of the input array.  It
is possible to fuse a producer that maps over one slice with a
consumer that maps over a different slice, and the result will be a
special \texttt{with}-loop (using an internal representation with
\texttt{genarray}s) that has different operations for different slices
of the index space.  This seems much like simply having branches
inside the functions, although perhaps this fuses better.

A \texttt{With}-loop compilation scheme is presented that tries to
exhibit good spatial locality.

\begin{quote}
  \fullcite{halvorsen2012calculating}
\end{quote}

Esben Halvorsens paper on basic multivariate calculus.  This
exposition is actually pretty good.  There is one dubious sentence:

\begin{quote}
  It could also be that it is simply more descriptive for the
  fluctuations of a pricing function to know the change in value over
  a larger amount of time, rather than at an instant.
\end{quote}

This seems a bit of a stretch.

The main contribution of the paper is a (correct) Haskell
implementation of an automatic differentiation scheme presented in
another paper.  The approach is based on overloading arithmetic
operations so that they do not compute results as such, but instead
create an infinite (lazy) tree of derivatives instead.

To implement automatic differentiation in Haskell, instances of the
numeric type classes are defined.  This is a classic trick in Haskell,
but really just a Haskell hack.  It breaks down rapidly (the author
notes an incomplete \texttt{Ord} definition for functions), but
permits a nicer syntax.

I am not a fan of the type classes imposed on functions.  I do not
believe the former is easier to read than the latter: It also risks
type safety, as suddenly every number becomes a constant function from
an arbitrary argument to itself.

\begin{verbatim}
*> exp*sin(cos + 2*id + 1) $ 3
-5.418932433500644
*> (\x -> exp x*sin(cos x + 2*x + 1)) 3
-5.418932433500644
*> 1 "foo"
1
\end{verbatim}

A big question is whether it is possible to use this approach to
generate code from the derivatives, rather than Haskell
(pseudo-)functions.  This runs into the same issues as other language
embeddings (see Accelerate).  In this case, one could perhaps hide the
Haskell standard library and only expose "safe" primitives.  It must
be more abstract!

\begin{quote}
  \fullcite{Blelloch:1996:PTS:232629.232650}
\end{quote}

The ``main'' NESL paper, or at least the one that presents the
rigorous cost model (based on \textit{depth}, \textit{work}, and
\textit{sequential space}) that is perhaps the most defining trait of
NESL from an academic perspective.  The idea behind the ``provably
efficient'' implementation is to assign each language construct a
cost, and prove that the mapping to a concrete machine maintains those
costs.  I assume that this doesn't mean ``provable'' with respect to
the actual code implementing the compilation (which is probably still
an unsolved problem).  This idea has been developed in an earlier
paper, but the application to NESL contains two novelties:

\begin{itemize}
\item Arrays and certain array operations are considered primitive, as
  they cannot be efficiently emulated.
\item Space is now part of the cost measure.
\end{itemize}

The time part of the cost model consists of a DAG representing
sequential control dependencies.  The number of nodes corresponds to
the amount of work, and the depth of the DAG to the work depth.

The space part is ``an accounting of the reachable space used by a
sequential implementation of the language''.

The paper uses an intermediate language with explicit array allocation
into which C0re-NESL is translated.  This array language is processed
by a machine, P-CEK($q$), which works by keeping a queue of
threads/states that are ready to execute.  On each step, the machine
executes the first $q$ of these threads.  Thus, $q$ represents the
parallelism of the machine.  This smells a lot like task parallelism.
The P-CEK($q$) machine is mapped to concrete hardware to validate the
entire cost model.  In the end, it is shown that a NESL program with
work $w$, depth $d$, and using $s$ sequential space will run in $O(w/p
+ d \log(p))$ time and $O(s + d p \log(p))$ space, on $p$ processors
of either a butterfly network (?), hypercube, or CRCW PRAM.  One worry
I have is that the cost model is incompatible with an efficient
compilation to vector hardware (i.e. GPU).  Interestingly, it is
mentioned that the implementation in the paper does not correspond to
their extant NESL implementations, which used flattening.  I wonder if
any implementations were ever made using this provably efficient cost
model.

NESL is a simple functional language - apart from I/O and random
number generation, it is pure.  It supports polymorphic types,
higher-order functions, and is call-by-value.  Parallelism is explicit
via parallel mapping and other primitives.  NESL has built-in support
for irregular arrays.  In the Core-NESL used in the paper, scan is a
primitive operation supported for only two operators (maximum and
addition).

The cost model makes use of a mutable store - interesting choice for
an eagerly evaluated pure language.  Evaluation takes an environment
$E$, a store $\sigma$, a set of ``root locations'' $R$, and evaluates
the expression $e$, yielding a value, a new store $\sigma'$, and a
computation DAG $g$ and space cost measure $s$.  I think the $R$
points at all the memory that may be used in the future, and is used
solely to compute the space cost measure.  This can be seen in Figure
4, where a function $space(R,\sigma)$ is defined.  This function
morally computes the size of the set of all values reachable from $R$
in $\sigma$, and is used in the evaluation relation on Figure 2.

Core-NESL is compiled to an imperative array language using fork-join
parallelism (at least the fork part - the join is implicit).  One fork
instruction can start any number of threads (given by an integer
argument).  The same evaluation judgement is used as for Core-NESL.  A
theorem is given that states that if a Core-NESL expression can be
evaluated to some result $v$ with dependency graph $g$ and space
requirement $s$, then the corresponding array language expression can
be evaluated to the same $v$, with a dependency graph that is at most
a constant greater than $g$, and space cost that is at most the same
constant greater than $s$.

The array language (why didn't they give it a name?!) is also what is
executed by the P-CEK($q$) machine.  The evaluation relation takes a
store and a queue of substates, and produces a new store (actually it
looks more like a store update) and queue of substates.  A substate is
a triple $(C,E,K)$ which, together with the store, corresponds roughly
to a sequential machine state (CESK), with $C$ an expression, $E$ the
environment, and $K$ the continuation.  A continuation is either
empty, a function argument to be evaluated, a function to be applied,
or a finishing/finished thread (which contains a value, or rather, a
location into the store).

When evaluating, each substate can transition into a single new
substate, fork off a group of new substates, or terminate.  New
(forked) substates are added add the \textit{front} of the queue(?),
maintaining their order relative to the original substates.  The
evaluation function for P-CEK($q$) is nondeterministic for $q > 1$
(remember that $q$ is the degree of parallelism in the machine), and
it is possible to have race conditions.  The authors claim that this
does not happen with any of the array programs created by the
translation from Core-NESL (and I believe them).  It is quite
interesting that they explicitly mention that the ability to scale the
amount of parallelism with the $q$-parameter is important to
preserving efficiency (they mention space bounds).  Actually, the
theoretical treatment would fall apart if they did not.  Yet still, so
many practical implementations of parallel languages dial up the
parallelism to useless and counterproductive levels.  It is amusing
that such a practical performance consideration only shows up in the
formal treatment.

Figure 8 shows the P-CEK($q$) evaluation relation.  I find it fairly
difficult to read.  The case for \texttt{scanadd} is particularly
mystifying.

The trick behind making this perform (in particular, $O(n)$ work
scans) is the use of the \textit{fetch-and-add}-operation, also called
multiprefix.  In this operation, each processors know an address and
has an integer value $i$.  In parallel, all processors can
automatically fetch the value from the address, whilst incrementing
the value by $i$.  The add is stable - if two processors make the
request simultaneously, the one with the smaller ID will access the
counter first.  I am skeptical about the efficient execution of this
on contemporary hardware.

In the end, this is a theoretical paper.  It is argued that this
provably efficient implementation is possible, but I have not found an
empirical demonstration.  This is a shame, because the approach is
very elegant.  The authors do address some practical implementation
concerns at the end - notably the clustering of computation into
larger blocks - but I am still skeptical.  This may perform well
enough on multicore hardware, but it seems like it would be hard to
make the necessary transformations to produce efficient vectorised
code.

\begin{quote}
  \fullcite{Panchekha:2015:AIA:2813885.2737959}
\end{quote}

This is an amazingly well-written paper.  One of the most pleasurable
reads I've had in a while.  It is even artifact-evaluated!  Top notch
science here.

The basic idea is to take a floating-point expression, generate a
number of input points (256 in practice), and for each input point
determine the difference between an arbitrary-precision computation
and the fixed-precision floating-point computation, which then
constitutes the error.  Herbie will evaluate the subexpressions of the
original expression to determine where the error is introduced (that
is, it's stochastic, not based on static analysis).  Rewrite rules are
then applied to rewrite the erroneous expression to be more accurate -
possibly only for certain ranges of the input.

At all times, Herbie keeps a table of ``programs'' (really full
expressions) - each of these is the most accurate for at least one of
the input points.  Herbie extraccts one of these programs, then finds
all ``locations'' inside of it - I assume this means pointers to
subexpressions - and sort these by their local error, taking only the
$M$ most erroneous.  Herbie then invokes a procedure called
\texttt{recursive-rewrite}, which is given the program and the
locations, and produces some more programs that are added to the
table.  This procedure uses rewrite rules to improve accuracy.  Also,
Herbie uses a similar procedure called \texttt{series-expansion}, that
has the same interface, but tries to improve accuracy by
series-expanding expressions.  This is all from Figure 2.

I don't understand the definition of \texttt{recursive-rewrite} in
Figure 4.  It does not seem to take a list of locations as its second
argument.

Invalid rewrite rules do not matter, because an invalid rule cannot
improve accuracy, hence they will be discarded as simply not an
improvement.  That's a cool detail!

The authors mention that they must avoid false identities such as
$\sqrt{x^{2}} = x$, which is only valid for non-negative $x$.  But
surely such a rule could still be useful, as long as the final regime
inference ensures that this version of the code is only used exactly
when $0\leq x$?  It seems to me that it would, by the very virtue of
the expression being very inaccurate for negative $x$.

Overhead of generated code compared to the original is 40\% - but
presumably this is only when changes are actually made, which in a
large program is likely to only be a small fraction of the total code.
The increased number of branches may hit harder on a vector machine.

\begin{quote}
  \fullcite{Lu:2010:SEP:1869389.1869392}
\end{quote}

The paper presents a GPU library for extended precision floating point
numbers (greater than 64 bits).  Clearly this is less efficient than
native numbers, but sometimes you gotta do what you gotta do, and the
slowdown they get is about the same as that on a CPU (actually a
little better it seems).  There are two forms of extended precision:

\begin{description}
\item[Multi-term,] in which the extended number is the sum of a
  statically known number of native precision floats.  Implemented in
  a library called QD, whose algorithms are used in this paper.  The
  paper implements double-double (128 bits) and quad-double (256 bits)
  numbers.
\item[Multi-digit,] in which an array of digits is stored alongside a
  single exponent.  This permits dynamic sizing of the number, like
  common bignum integer implementations.  Implemented in a library
  called ARPREC, whose algorithms are used in this paper.
\end{description}

I am totally confused by Algorithm 1.  What is $\oplus$ and $\ominus$?
I think they are just plus and addition on floating-point numbers.

The paper discusses not just the representation of single floats, but
also array of floats, which are stored in transposed form to permit
memory coalescing (see Figure 2 in the paper).  Strangely, they only
report a factor-of-3 speedup, but if they only use four words per
float, (corresponding to double-doubles), then this is about right:
for a 16-thread warp, four of the 16 threads would still access within
the same 16-word memory region.

They talk about using local memory to offset register pressure, which
sounds... wrong.

In total, the paper is a fairly straightforward port of established
extended-precision algorithms to the GPU.  Most of the paper consists
of a fairly in-depth evaluation section that strongly supports the
idea, and shows that performance is very good on the GPU (although I
would have liked to see some comparisons to native floats).

The code for QDR is located at
\url{https://code.google.com/p/gpuprec/}.  I have been unable to
locate the source for GARPREC, even though the repository claims to
contain it.

I did my own experiments on my Futhark implementation.  For computing
\texttt{zipWith(+,as,bs)}, where arrays contain three million
quad-doubles, I get the following runtimes:

\begin{tabular}{l|l}
  \hline
  \textbf{Machine} & \textbf{Runtime} \\\hline
  GeForce GTX 690 (\texttt{napoleon}) & 1794us \\
  Sequential CPU (\texttt{napoleon}) & 24859us \\
\end{tabular}

This is a speedup of 13.47 over \textit{sequential} performance.  Note
that the GPU has atrocious double-precision performance.

\begin{tabular}{l|l}
  \hline
  Machine & Runtime \\\hline
  AMD FirePro W8100 (\texttt{napoleon}) & 849us (best)
\end{tabular}

Surprisingly, the AMD GPU does little better, despite being much more
powerful for double-precision.

I suspect this is because we are bandwidth-bound anyway.  Let us
calculate: The GTX 690 has a memory bandwidth of 192GiB/s (for one
GPU).  It has single-precision performance of 5,621 GFLOPS, and
double-precision at 1/24th (!!!) of that, which is 223 GFLOPS (come
on).  This means you can deliver slightly less than one byte per
instruction, or roughly one double for every eight instructions, or
roughly two quad-doubles for every 64 instructions.  The question is
then whether it takes more than 64 instructions to perform the add (it
does), so it doesn't look memory-bound.  Of course, not all the
operations are double-precicion, but the only integer operations are
some simple comparisons and branches.

The W8100 has 2,620 GFLOPS of double-precision performance and 320GB/s
of memory bandwidth, which is one byte for about every eight
instructions, or one double for every 64 instructions, or two
quad-doubles for every 512 instructions.  Now, this may very well be
quite memory-bound.  The W8100 has about 66\% more memory bandwidth
than the GTX 690, and obtains a speedup of around 2.

Let us consider the GPU in the paper: the GTX 280.  The paper reports
a memory bandwidth of 110GiB/s and 90GFLOPS in double precision.  This
leaves one byte for every instruction, and two quad-doubles every 64
instructions.  Not particularly bandwidth-bound.  In the paper, they
use a Q6600 CPU, which is 2007-era, whereas the GTX 280 is 2008-era.
Not a big difference.

Some more data:

\begin{tabular}{l|l}
  \hline
  Machine & Runtime \\\hline
  GTX 780 Ti (\texttt{gpu01-diku-apl}) & 820us
\end{tabular}

This is despite the GTX 670 Ti only being rated at 210GFLOPS in
double-precision.

Let us try dot product!  Same input data.

\begin{tabular}{l|l}
  \hline
  Machine & Runtime \\\hline
  AMD FirePro W8100 (\texttt{napoleon}) & 6314us \\
  GTX 780 Ti (\texttt{gpu01-diku-apl}) & 5163us \\
  Sequential CPU (\texttt{napoleon}) & 61001us \\
\end{tabular}

Now, these are total runtimes, but the actual reduce kernel is much
faster on the W8100 (765us) than on the GTX 780 Ti (1721us).  No idea
why the total runtimes are so different.

\begin{quote}
  \fullcite{Ragan-Kelley:2013:HLC:2491956.2462176}
\end{quote}

\begin{quote}
  \fullcite{Fahndrich:2002:AFP:543552.512532}
\end{quote}

\begin{quote}
  \fullcite{Tov:2011:PAT:1926385.1926436}
\end{quote}

\begin{quote}
  \fullcite{Smetsers94guaranteeingsafe}
\end{quote}

Seems like a very similar approach to what we're doing in Futhark,
although with a more complicated language.  Key quote:

\begin{quote}
  However, in some speciac cases destructive updates are safe,
  e.g. when it is known that access on the original object is not
  necessary in the future. We call such an object (locally) `unique'.
\end{quote}

The paper describes how uniqueness types are implemented in Concurrent
Clean, and how they are used to handle in-place updates.  Papers on
the same subject were used to originally develop the uniqueness types
in Futhark in 2013.

\begin{quote}
  \fullcite{Leshchinskiy:2009:RYA:1506052.1506073}
\end{quote}

Fusion of bulk array updates.

\begin{quote}
  \fullcite{moseley2006out}
\end{quote}

Key sentence:

\begin{quote}
  The bottom line is that the more powerful a language (i.e. the more
  that is possible within the language), the harder it is to
  understand systems constructed in it.
\end{quote}

This also applies to how easily a compiler can optimise a program
written in the language!

\begin{quote}
  \fullcite{Dubach:2012:CHL:2345156.2254066}
\end{quote}

Presents Lime, a ``Java-compatible language'', that targets execution
on heterogenous systems.  It looks like an extension of Java, with
build-in map/reduce facilities.  Syntactically and type-wise, nested
parallelism seems to be permitted.  The programmer must indicate
specific \textit{tasks}, in which the use of the map and reduce
operators are detected by the compiler.  Lime maps some arrays (that
are small) to private memory.  It caches in local memory using a
technique similar to our loop tiling.  It may be somewhat more
limited, as I'm not sure it does variance analysis.  Maybe won't work
for matrix multiplication or LavaMD.  Lime also uses image/texture-
and constant memory, which is pretty cool!

Tested on three benchmarks from Parboil (CP, MRI-Q, RPES), two from
Java Grande (Crypt and Series), and two hand-written (N-Body and
Mosaic).

It is not clear whether Lime exploits nested parallelism, but none of
the benchmarks contain it.  Nothing on fusion.

This paper is very well written.  It should be a model for study.

\begin{quote}
  \fullcite{Steuwer:2017:LFD:3049832.3049841}
\end{quote}

A paper on the OpenCL-specific variant of the Lift IR, on which
OpenCL-aware optimisations can be performed.  The IR is used to
represent single OpenCL kernels.  Does not use a fixed strategy for
optimisation.  The paper is about how the IR is compiled to actual
code, as well as a demonstration that the IR is flexible enough to
represent certain hand-written kernels.  At some point, higher-level
IR is translated to this, perhaps by a method similar to Futhark's
kernel extraction.  Like Futhark, Lift is also based on the lambda
calculus.  Application-specific scalar code embedded as C functions.
A special \texttt{slide} construct used for stencils (but without
host-level code for the time loop, how can this be optimised?).  Lift
OpenCL IR is significantly more low-level than Futhark, but could
conceivably be used as a target for our kernel extraction algorithm.
They have \textit{views}, which serve a purpose similar to our index
functions.  Views permit an array to have several underlying memory
blocks; index functions do not.  They use this to implement
\texttt{zip}, which we do with source code transformation.  The Lift
\texttt{toLocal} pattern is similar to our \texttt{combine}.  They do
double buffering with point swapping---embarassingly, we do not.

\begin{quote}
  \fullcite{Cunningham:2011:GPH:2212736.2212744}
\end{quote}

A paper on compiling X10 (an IBM PGAS language, like Chapel) to CUDA.
X10 is not itself a GPU language, and seems to focus on distributed
and heterogeneous computing.  More control than e.g. Futhark.  They
call it ``APGAS'', not just ``PGAS''.  Threads, blocks, barriers,
constant memory, etc. can be expressed directly in X10.  In fact,
\textit{must} be---code must be GPU-aware.  Cleanly handles clusters
of GPUs.  Memory must be pre-allocated using special functions, and
manually freed.  No nested parallelism.  Programmers must be aware of
all low-level performance effects, e.g. coalescing and bank conflicts.
No high-level optimisations; only constant folding is mentioned.  In
X10 terms, each GPU is one ``place''.

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
